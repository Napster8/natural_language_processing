{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f686a9a8-61e3-450b-935b-9c2a6060e96d",
   "metadata": {},
   "source": [
    "## Text Pre-Processing:\n",
    "\n",
    "1. Tokenizing words.\n",
    "2. Normalizing word formats.\n",
    "3. Segmenting sentences.\n",
    "\n",
    "### Tokenizing words:\n",
    "\n",
    "Problem with space based Tokenization is that it blindly removes punctuations:\n",
    "1. m.p.h, Ph.D, AT&T, cap'n\n",
    "2. prices ($45.55)\n",
    "3. Dates (01/02/06)\n",
    "4. URL (http://www.google.com)\n",
    "5. hashtags (#nlp)\n",
    "6. email addresses (abc@gmail.com)\n",
    "7. Clitic: a word that does not stand on its own.\n",
    "    1. 'are' in we're.\n",
    "    2. French 'je' in j'ai.\n",
    "    3. 'le' in l'honneur.\n",
    "8. When should multi word expressions (MWE) be words?\n",
    "    1. New York\n",
    "    2. Rock 'n' roll\n",
    "9. Many languages like Chinese, Japanese, Thai don't use spaces to seperate words.\n",
    "\n",
    "How do we decide where the token boundaries should be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243eb08-f6fa-489d-8454-908716a3e3c2",
   "metadata": {},
   "source": [
    "## Method 1: Byte Pair Encoding (BPE)\n",
    "\n",
    "BPE is a simple form of data compression algorithm in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur in that data.\n",
    "\n",
    "BPE brings the perfect balance between character- and word-level hybrid representations which makes it capable of managing large.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Let vocabulary be the set of all individual characters. = ${a,b, c, d} $\n",
    "2. Repeat:\n",
    "    1. Choose the two characters that occur most frequently adjacent in the training corpus (say 'a', 'b')\n",
    "    2. Add a new merged symbol 'ab' to the vocabulary.\n",
    "    3. Replace every adjacent 'a' 'b' in the corpus with 'ab'.\n",
    "3. Until k merges have been done.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. Includes most frequently occuring words and sub-words.\n",
    "2. It picks morphemes. A morpheme is the smallest meaning-bearing unit of a language.\n",
    "3. unlikeliest has 3 morphemes un-, likely and -est.\n",
    "4. This behavior also enables the encoding of any rare words in the vocabulary with appropriate subword tokens without introducing any “unknown” tokens.\n",
    "5. It does not really require one to understand the language to tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65be4698-dd80-43a6-a8bf-4cf73b9946a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'low', 'low', 'low', 'lowest', 'lowest', 'lowest', 'newer', 'newer', 'new', 'newer', 'wider', 'wide', 'new', 'lower']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "def bert_tokenizer(word):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    print(tokenizer.tokenize(word))\n",
    "\n",
    "corpus = \"low low low low lowest lowest lowest newer newer new newer wider wide new lower\"\n",
    "bert_tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51cc416f-162f-4248-bdb4-8529e617efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'gp', '##u', 'good', 'deal', ',', 'even', 'more', 'good']\n"
     ]
    }
   ],
   "source": [
    "corpus = \"New GPU good deal, even more good\"\n",
    "bert_tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecaf90f6-13ba-4c87-abd6-86cdf29bc292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁New', '▁G', 'PU', '▁good', '▁deal', ',', '▁even', '▁more', '▁good']\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "\n",
    "def xltokenizer(word):\n",
    "    tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "    print(tokenizer.tokenize(word))\n",
    "\n",
    "corpus = \"New GPU good deal, even more good\"\n",
    "xltokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9a8c294-c2d4-4382-91cf-26987394604d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁low', '▁low', '▁low', '▁low', '▁lowest', '▁lowest', '▁lowest', '▁newer', '▁newer', '▁new', '▁newer', '▁wider', '▁wide', '▁new', '▁lower']\n"
     ]
    }
   ],
   "source": [
    "corpus = \"low low low low lowest lowest lowest newer newer new newer wider wide new lower\"\n",
    "xltokenizer(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd9d8e-f044-4f79-9259-62f0f14bc840",
   "metadata": {},
   "source": [
    "## Lemmatization and Stemming - Word Normalization\n",
    "\n",
    "Lemmatization is finding out the shared roots of several words in the corpus.\n",
    "\n",
    "Advantage is that it reduces alot of task dependent noise in the data. Disadvantage is that for specific NLP tasks such as machine translation, we need to preserve those original words rather than using lemmatized forms.\n",
    "\n",
    "am, are, is --> be\n",
    "\n",
    "car, cars, car's, cars' --> car\n",
    "\n",
    "He is reading detective stories --> He be read detective story.\n",
    "\n",
    "Disadvantage:\n",
    "1. Contexts are lost by this normalization technique.\n",
    "2. Language Dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd210b3-c700-40fc-8e9e-ebc0c0aca47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba67395f-054c-429a-aa0a-b439731e3140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTt\t\t|\tLEMMA\t|\tPOS\t|\tIS_STOP_WORD\n",
      "Reading\t\t|\tread\t|\tVERB\t|\tFalse\n",
      "books\t\t|\tbook\t|\tNOUN\t|\tFalse\n",
      "with\t\t|\twith\t|\tADP\t|\tTrue\n",
      "people\t\t|\tpeople\t|\tNOUN\t|\tFalse\n",
      "while\t\t|\twhile\t|\tSCONJ\t|\tTrue\n",
      "eating\t\t|\teat\t|\tVERB\t|\tFalse\n",
      "lunch\t\t|\tlunch\t|\tNOUN\t|\tFalse\n",
      ".\t\t|\t.\t|\tPUNCT\t|\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "def lemmatize(corpus):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    lemmatized_list = nlp(corpus)\n",
    "    \n",
    "    # Token, lemmatized format, Which parts of speech, is it a stop word or not\n",
    "    print(f'TEXTt\\t\\t|\\tLEMMA\\t|\\tPOS\\t|\\tIS_STOP_WORD')\n",
    "    for token in lemmatized_list:\n",
    "        print(f'{token.text}\\t\\t|\\t{token.lemma_}\\t|\\t{token.pos_}\\t|\\t{token.is_stop}')\n",
    "\n",
    "corpus = '''Reading books with people while eating lunch.'''\n",
    "lemmatize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0573c01-af14-4787-90d8-63e0581bd1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>terrible place to work for i just heard a stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>hours , minutes total time for an extremely s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>my less than stellar review is for service . w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>i m granting one star because there s no way t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>the food here is mediocre at best . i went aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review\n",
       "0  negative  terrible place to work for i just heard a stor...\n",
       "1  negative   hours , minutes total time for an extremely s...\n",
       "2  negative  my less than stellar review is for service . w...\n",
       "3  negative  i m granting one star because there s no way t...\n",
       "4  negative  the food here is mediocre at best . i went aft..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reviews dataset\n",
    "df = pd.read_csv('reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ff6616e-3637-4b71-b09b-5ee79b55483a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hours , minutes total time for an extremely simple physical . stay away unless you have hours to waste ! ! ! '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = np.asarray(df['review'])\n",
    "reviews[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a475a13b-1358-482a-a5b1-f4a5456662b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXTt\t\t|\tLEMMA\t|\tPOS\t|\tIS_STOP_WORD\n",
      " \t\t|\t \t|\tSPACE\t|\tFalse\n",
      "hours\t\t|\thour\t|\tNOUN\t|\tFalse\n",
      ",\t\t|\t,\t|\tPUNCT\t|\tFalse\n",
      "minutes\t\t|\tminute\t|\tNOUN\t|\tFalse\n",
      "total\t\t|\ttotal\t|\tADJ\t|\tFalse\n",
      "time\t\t|\ttime\t|\tNOUN\t|\tFalse\n",
      "for\t\t|\tfor\t|\tADP\t|\tTrue\n",
      "an\t\t|\tan\t|\tDET\t|\tTrue\n",
      "extremely\t\t|\textremely\t|\tADV\t|\tFalse\n",
      "simple\t\t|\tsimple\t|\tADJ\t|\tFalse\n",
      "physical\t\t|\tphysical\t|\tNOUN\t|\tFalse\n",
      ".\t\t|\t.\t|\tPUNCT\t|\tFalse\n",
      "stay\t\t|\tstay\t|\tVERB\t|\tFalse\n",
      "away\t\t|\taway\t|\tADV\t|\tFalse\n",
      "unless\t\t|\tunless\t|\tSCONJ\t|\tTrue\n",
      "you\t\t|\t-PRON-\t|\tPRON\t|\tTrue\n",
      "have\t\t|\thave\t|\tAUX\t|\tTrue\n",
      "hours\t\t|\thour\t|\tNOUN\t|\tFalse\n",
      "to\t\t|\tto\t|\tPART\t|\tTrue\n",
      "waste\t\t|\twaste\t|\tVERB\t|\tFalse\n",
      "!\t\t|\t!\t|\tPUNCT\t|\tFalse\n",
      "!\t\t|\t!\t|\tPUNCT\t|\tFalse\n",
      "!\t\t|\t!\t|\tPUNCT\t|\tFalse\n"
     ]
    }
   ],
   "source": [
    "corpus = reviews[1]\n",
    "lemmatize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3f5ad-ac0e-4158-9cc3-47aa964c86e4",
   "metadata": {},
   "source": [
    "### Stemming - word Normalization\n",
    "\n",
    "A stemming algorithm is a computational procedure which reduces all words with the same root… to a common form, usually by stripping each word of its derivational and inflectional suffixes. The stem obtained may or may not be linguistically correct/meaningful.\n",
    "\n",
    "The `Spacy library` does not have stemming operation. It relies on lemmatization.\n",
    "But NLTK has stemming. Two types:\n",
    "1. Porter Stemmer\n",
    "2. Snowball Stemmer\n",
    "\n",
    "Snowball stemmer is a slightly improved version of the Porter stemmer and is usually preferred over the latter. \n",
    "\n",
    "    1. It supports multiple languages, including English, Russian, Danish, French, Finnish, German, Italian, Hungarian, Portuguese, Norwegian, Swedish, and Spanish. \n",
    "\n",
    "Examples\n",
    "1. ATIONAL --> ATE (relational --> relate)\n",
    "2. motoring --> motor\n",
    "3. grasses --> grass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdede18e-3744-4c0c-82c7-4bddad513645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tcompute --> comput\n",
      "2\tcompute --> comput\n",
      "---------------------------\n",
      "1\tcomputer --> comput\n",
      "2\tcomputer --> comput\n",
      "---------------------------\n",
      "1\tcomputed --> comput\n",
      "2\tcomputed --> comput\n",
      "---------------------------\n",
      "1\tcomputing --> comput\n",
      "2\tcomputing --> comput\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer_1 = PorterStemmer()\n",
    "stemmer_2 = SnowballStemmer(language='english')\n",
    "\n",
    "tokens = ['compute', 'computer', 'computed', 'computing']\n",
    "for token in tokens:\n",
    "    print('1\\t' + token + ' --> ' + stemmer_1.stem(token))\n",
    "    print('2\\t' + token + ' --> ' + stemmer_2.stem(token))\n",
    "    print('---------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d750759-a418-4119-aa10-ef5e56522b8a",
   "metadata": {},
   "source": [
    "### Extra Feature of Snowball Stemming\n",
    "The stemming of stopwords like ‘being’ to ‘be’ is useless because they don’t have any shared meaning, although there could be a grammatical connection between these two words. Snowball provides another parameter called `ignore_stopwords`, which is set to false by default. \n",
    "\n",
    "If it is set to true, then snowball will not perform the stemming of stopwords.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c408336-208a-470b-b5f8-c6149bbda7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer 1: is --> is\n",
      "Stemmer 2: is --> is\n",
      "Stemmer 3: is --> is\n",
      "---------------------------\n",
      "Stemmer 1: being --> be\n",
      "Stemmer 2: being --> be\n",
      "Stemmer 3: being --> being\n",
      "---------------------------\n",
      "Stemmer 1: be --> be\n",
      "Stemmer 2: be --> be\n",
      "Stemmer 3: be --> be\n",
      "---------------------------\n",
      "Stemmer 1: was --> wa\n",
      "Stemmer 2: was --> was\n",
      "Stemmer 3: was --> was\n",
      "---------------------------\n",
      "Stemmer 1: founding --> found\n",
      "Stemmer 2: founding --> found\n",
      "Stemmer 3: founding --> found\n",
      "---------------------------\n",
      "Stemmer 1: mice --> mice\n",
      "Stemmer 2: mice --> mice\n",
      "Stemmer 3: mice --> mice\n",
      "---------------------------\n",
      "Stemmer 1: runs --> run\n",
      "Stemmer 2: runs --> run\n",
      "Stemmer 3: runs --> run\n",
      "---------------------------\n",
      "Stemmer 1: running --> run\n",
      "Stemmer 2: running --> run\n",
      "Stemmer 3: running --> run\n",
      "---------------------------\n",
      "Stemmer 1: ran --> ran\n",
      "Stemmer 2: ran --> ran\n",
      "Stemmer 3: ran --> ran\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "tokens = ['is', 'being', 'be', 'was', 'founding', 'mice', 'runs', 'running', 'ran']\n",
    "stemmer_1 = PorterStemmer()\n",
    "stemmer_2 = SnowballStemmer(language='english')\n",
    "stemmer_3 = SnowballStemmer(language='english', ignore_stopwords=True)\n",
    "\n",
    "for token in tokens:\n",
    "    print('Stemmer 1: ' + token + ' --> ' + stemmer_1.stem(token))\n",
    "    print('Stemmer 2: ' + token + ' --> ' + stemmer_2.stem(token))\n",
    "    print('Stemmer 3: ' + token + ' --> ' + stemmer_3.stem(token))\n",
    "    print('---------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d75bf-9513-4c74-99b0-d10499551ff4",
   "metadata": {},
   "source": [
    "## Case Folding - Normalizing Step\n",
    "\n",
    "Putting tokens/ words into a standard format.\n",
    "\n",
    "1. U.S or US\n",
    "2. uhhuh or uh-huh\n",
    "3. Fed or fed\n",
    "4. am, is, be, are\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f815b51-2ba1-4764-90f0-1f516bf7eb68",
   "metadata": {},
   "source": [
    "### Text Pre-Processing Step - Spelling Correction \n",
    "\n",
    "Minimum Edit Distance\n",
    "\n",
    "If each operation has a cost of 1\n",
    "\n",
    "    * Distance between these is 5.\n",
    "If substituion cost is 2 (Levenshtein)\n",
    "\n",
    "    * Distance between them is 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d2fa024-7d67-4049-b8a0-8e8099ca7e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import editdistance\n",
    "from english_words import english_words_set\n",
    "corpus = ['banana', 'bahana']\n",
    "editdistance.eval('banana', 'bahama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bdd9502-3310-45b4-ba2a-e06e6b755114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(word, vocabulary):\n",
    "    '''\n",
    "    Spelling correction is done by checking the minimum edit distance(med)\n",
    "    '''\n",
    "    med = [editdistance.eval(word, vocabulary[i]) for i in range(len(vocabulary))]\n",
    "    return vocabulary[np.argmin(med)]\n",
    "\n",
    "def correct_the_spell(word = 'banama'):\n",
    "    # Default Vocabulary from spacy\n",
    "    # nlp = spacy.load('en_core_web_sm')\n",
    "    # vocabulary = list(nlp.vocab.strings)\n",
    "    \n",
    "    # English Vocabulary\n",
    "    vocabulary = list(english_words_set)\n",
    "\n",
    "    print(f'Original Word: {word}')\n",
    "    print(f'The correct spelling: {spelling_correction(word, vocabulary)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee597855-1db2-4556-ba93-9177c862633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: banama\n",
      "The correct spelling: panama\n"
     ]
    }
   ],
   "source": [
    "correct_the_spell(word = 'banama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07b048cf-62e8-4962-a8cc-d4bb1dfa8e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: bahana\n",
      "The correct spelling: banana\n"
     ]
    }
   ],
   "source": [
    "correct_the_spell(word = 'bahana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbca78c9-4632-44b1-9cbe-f7bf96d26d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: ghosr\n",
      "The correct spelling: ghost\n"
     ]
    }
   ],
   "source": [
    "correct_the_spell(word = 'ghosr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1178470f-9400-4d85-9e27-0ee4d812d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: acceptible\n",
      "The correct spelling: accessible\n"
     ]
    }
   ],
   "source": [
    "correct_the_spell(word = 'acceptible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc938c21-1727-47c5-9e26-c164857b376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: hight\n",
      "The correct spelling: eight\n"
     ]
    }
   ],
   "source": [
    "correct_the_spell(word = 'hight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb92f89-5eab-4ae3-ab44-fec54a425f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
